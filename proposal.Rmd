---
title: "Data Expedition Proposal"
author: "Becky Tang and Graham Tierney"
date: "7/23/2019"
output:
  pdf_document: default
  html_document: default
---

1. Sponsoring faculty member: 
  + Fan Li (STA 440)
  + Maria Tackett (STA 199)
  
2. Title of dataset(s)
  + 
  + State of the Union Transcripts, 1970-present
  
3. Description:
  + 
    + One-two sentence description of data file 
    + Source(s)
    + Why this dataset?
    + How dataset was put together
    + Dimensions of dataset
  + State of the Union Transcripts, 1970-present
    + The data associated with this project are transcripts from State of the Union addresses beginning with President Nixon's 1970 address. Also included is an Excel file `stateofunion.xlsx' specifying the year, president, and the president's political party corresponding to each address. 
    + The data consist of 50 plain text files. The transcripts vary in length/size, with the 1973nixon.txt file notably larger than the other transcripts. In 1973 President Nixon gave six separate addresses, so we decided to concatenate these addresses into one large file. 
    + The transcripts were obtained from the website The American Presidency Project. We simply copy-and-pasted each transcript into a .txt file, although in the future we could use a web scraper. 
    + Motivation: Graham and I recently read \textit{The New Jim Crow: Mass Incarceration in the Age of Colorblindness}, and one major theme presented in the book is that mass incarceration in America began when President Nixon coined the term ``War on Drugs''. We are interested in exploring this topic further, and see the benefits of connecting statistical methods with the social sciences. In terms of the choice of this specific text, we believe that State of the Union addresses lend themselves nicely to various forms of text analysis. These include topic modeling, classification to president or party, examining what policies were considered most important at the time of the speech based on word/topic frequencies (ex- how crime has been treated by the presidents over time), and much more.
    
4. Potential classroom exercises
  + With these datasets, we can ask questions at different levels of granularity:
    + Document specific (ex. What are the major themes/topics presented in Document Z? What is the overall sentiment associated with Document Z?)
    + Author/speaker specific (ex. How does Person X's language change over time? How does Person X feel about Issue Y?)
    + Comparisons across documents and authors (ex. How often does Person X speak about Issue Y in comparison to Person Y? How similar is the language used by different authors or in different documents? Does Person X use more or less positive language than Person Y? Given text from Document Z, can we determine who the author is?)
  + These sorts of questions fall into the two broad categories of sentiment analysis and topic modeling. We envision utilizing the tidytext R package to format the data. At the very base level, we can calculate raw word frequencies/proportions. We can then build up to correlation tests across authors/documents, difference in proportion tests, PCA analysis, and LDA (more details in the following bullet).
5. Techniques
  + Sentiment Analysis
    + Sentiment analysis provides a simple way to reduce the dimensionality of text data. Certain words are mapped to a given (short) list of sentiments. Each document is then scored by the number or proportion of words of the given sentiment. Some sentiment dictionaries map words to a continuous positive or negative scale, and then a document's sentiment is the average or total of the score. 
    + These scores can be easily visualized in histograms and density plots for different categories of documents or across different levels of some document feature (such as time of creation). 
    + The data can be explored with PCA using the sentiment scores to observe any clustering of documents. 
    + Logistic regression can be used to classify documents based on their sentiment scores, with k-fold cross-validation to assess accuracy. 
  + Multiple testing for word proportions
    + A method to identify words that are used “significantly” differently across authors.  
    + If documents in catgory A use word $i$ at frequency $p_{iA}$ and documents in category B use word $i$ at frequency $p_{iB}$, a simple two-sample t-test can be used to test the null hypothesis that $p_{iA} = p_{iB}$. If the null is rejected, then documents in category A are significantly different from documents in category B in this feature. 
    + However, since the number of unique words in the corpus is quite large, multiple testing corrections are necessary. The Bonferoni correction is useful if one is trying to assess whether there is any difference in word use between the categories. The Benjamini and Hochberg (1995) correction is more useful to identify the subset of words that differentiate the two categories. 
    + This method was used in Airoldi et al. (2006). 
  + Bag-of-words generative models: 
    + The general assumption of these approaches is that authors pick words according to some unknown set of frequencies. The likelihood of an observed word is thus proportional to the unknown frequency. The order of words is generally ignored in these models. 
    + With a Dirichlet prior on the frequency vector, the posterior can be easily computed via conjugacy. Students with more statistics background can be assigned to compute these posteriors and their properties as homework problems. For instance, the probably that a text with unknown authorship comes from a certain author is proportional to the prior probably times the posterior-predictive likelihood for that author (the multinomial-Dirichlet likelihood). This probability can be used to measure classification accuracy and quantification of uncertainty. 
    + This method generally requires labeled data from two or more authors/groups of authors. 
    + Some examples of this method in practice are Airoldi et al. (2006) and Gentzcow, Shapiro, and Taddy (2019).
  + Topic modeling with LDA
    + LDA is an unsupervised generative model that does not require labeled data. 
    + A “topic” is modeled by a frequency vector over all possible words. If a word $W$ comes from topic $t$, then $P(W = w_i) = \beta_{ti}$. 
    + For a fixed number of topics, a corpus of documents can be generated by the following process. Choose the length of the document (number of words) via $N \sim Pois(\lambda)$. Choose each document’s topic frequency by $\theta_d ~ Dir(\alpha)$. For each word $w_i$ in the document $d$, draw $z_{di} ~ \text{Categorical}(\theta_d)$, then draw word $w_i ~ \text{Categorical}(\beta_{z_i})$. 
    + Both full and collapsed Gibbs samplers can be programed by students who have taken 360. All results follow from conjugacy. 
    + After estimation, one can inspect the topic-word frequencies to see what concepts they relate to and the document-topic frequencies to see which document come mostly from each topic. Documents can be classified into the topic their words most frequently come from. 
  
6. Source(s)
  + State of the Union Address Transcripts obtained from The American Presidency Project (https://www.presidency.ucsb.edu/)