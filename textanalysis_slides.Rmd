---
title: "Text Analysis with R"
author: "Becky Tang and Graham Tierney"
date: "October 3, 2019"
output: ioslides_presentation
editor_options: 
  chunk_output_type: console
---
```{css echo=FALSE}
pre {
  max-height: 300px;
  float: center;
  width: 800px;
  overflow-y: auto;
}

pre.r {
  max-height: none;
}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
knitr::opts_chunk$set(class.source = "chunk-style")
library(tidyverse)
library(tidytext)
library(stringr)
library(tidyr)
library(scales)
library(textdata)
library(wordcloud)
library(reshape2)
library(lubridate)
library(widyr)
library(igraph)
library(ggraph)
```

# Introduction 
 

## What is text analysis?

- As statisticians and data scientistis, why restrict ourselves to numeric data?
- We can use text as data and employ statistical methods to derive patterns or trends 
  - Example: Given some song lyrics, can we determine who the singer is? What kinds of topics/sentiments does Lizzo like to sing about?
  
## Tidytext

- Thus far in class you have been employing tidy data principles, and we will continue to do so here!
- Specifically, we will work in the 'tidy text format': a table with one token per row
```{r introduce_text}
text <- c("Thus far in class you have been employing tidy data principles, and we will continue to do so here!", 
          "Specifically, we will work in the 'tidy text format': a table with one token per row")
text
```
- How can we make this text into a format we can work with?

---
```{r text-df}
text_df <- tibble(line = 1:2, text = text)
text_df
```

## Tokenizing

- A token is a meaningful unit of text. For us, that will be a single word
- Tokenizing is simply splitting a body of text into tokens, which can achieved using the function `unnest_tokens()`

```{r unnest-token, echo = T}
tidy_text <- text_df %>%
  unnest_tokens(word, text)
tidy_text
```

## Tokenizing cont.
- With the `unnest_tokens()` function, we can easily format any body of text into a user-friendly data frame
- First argument: name of column/variable that text is being unnested into
- Second argument: name of column/variable that is to be unnested 
- Other details:
  - All other columns kept
  - Punctuation removed
  - Defaults is to convert tokens to lowercase

# Twitter Data

## Democratic candidate tweets

- Time to work with some fun data!
- We have pulled tweets from four Democratic candidates: Joe Biden, Kamala Harris, Bernie Sanders, and Elizabeth Warren

```{r echo = F}
tweets <- read.csv("data/dem_cand_tweets_2019_10_02.csv")
#colnames(tweets)
tweets <- tweets %>%
  filter(!str_detect(text, "^RT")) %>%   #get rid of retweets
  mutate(timestamp = ymd_hms(created_at)) %>%
  select("timestamp","screen_name", "text")
tweets %>%
  select("screen_name", "text") %>%
  slice(1:3)
```

## Tidy tweets

```{r warning = F}
remove_reg <- "&amp;|&lt;|&gt;"
tidy_tweets <- tweets %>%
  mutate(text = str_remove_all(text, remove_reg))%>%
  unnest_tokens(word, text, token = "tweets")
tidy_tweets %>% slice(1:5)
```

## Word counts

- What are the most commonly tweeted words by these candidates?
```{r}
tidy_tweets %>%
  count(word, sort = T)
```

## Stop words
- Common/non-distinctive words

```{r, echo = T}
data("stop_words")
head(stop_words)
```

## Remove stop words

```{r}
tidy_tweets <- tidy_tweets %>%
  filter(!word %in% stop_words $ word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]"))
tidy_tweets %>%
  count(word, sort = T)
```

## Word frequency

- While it's nice to get raw counts, we may prefer to know which words are used more often \textit{relative} to other words. For this we can look towards word frequencies:

```{r frequency}
frequency_all <-  tidy_tweets %>%
  count(word, sort = T) %>%
  mutate(freq = n / sum(n)) 
```

## Frequency plot

```{r freq-plot, fig.height=3}
ggplot(frequency_all %>% top_n(10, freq) %>%
         mutate(word = reorder(word, freq)), aes(x = word, y = freq))+
  geom_col()+ 
  coord_flip() 
```

## Frequency by candidate

```{r}
frequency <- tidy_tweets %>%
  group_by(screen_name) %>%
  count(word, sort = T) %>%
  left_join(tidy_tweets %>%
              group_by(screen_name) %>%
              summarise(total = n()),
            by = "screen_name") %>%
  mutate(freq = n / total)
frequency %>% slice(1:4)
```

# Sentiment analysis

## Sentiment lexicons

- Now that we have looked at a per-word/token level, we may be interested in determining the general emotion of the text/author
- We can consider a word to have a particular emotion attached to it
- We can consider a piece of text as a combination of its individual words, so the sentiment of an entire text is the sum of the sentiments of its individual words
- There are three general-purpose sentiment lexicons: AFINN, bing, and nrc

## AFINN

- AFINN assigns each word an integer score between -5 and 5
- Negative scores indicate negative sentiment, and positive scores indicate the opposite

```{r}
get_sentiments("afinn") 
```

## bing

- The bing lexicon categorizes words into one of two categories: 'positive' or 'negative'
```{r}
get_sentiments("bing") 
```

## nrc

- The nrc lexicon categorizes words into one of 10 categories: anger, anticipation, disgust, fear, joy, negative, positive, sadness, surprise, or trust. 
```{r}
get_sentiments("nrc") 
```

## Important notes about sentiment lexicons

- Not every English word is in a lexicon
```{r}
get_sentiments("bing") %>%
  filter(word == "data")
```

- These lexicons do not account for qualifiers before a word (ex. "not happy") because these lexicons are constructed for one-word tokens only
- If we sum up each word's sentiment, it is possible for a piece of text to average out to a sentiment of zero/neutral

## Sentiment of Elizabeth Warren's tweets: bing lexicon
- We will use the bing lexicon to estimate the sentiment of Elizabeth Warren's tweets:

```{r}
tidy_tweets %>%
  filter(screen_name == "ewarren") %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(sentiment) %>%
  spread(sentiment, n) %>%
  mutate(sentiment = positive - negative)
```

- It appears that Warren is pretty neutral...or is she?

## Sentiment of Elizabeth Warren's tweets by month

- What if we look at Warren's tweets by month?

```{r}
tidy_tweets %>%
  filter(screen_name == "ewarren") %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(months(timestamp), sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

```

## Most common positive/negative words

```{r echo = T}
bing_counts <- tidy_tweets %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(word, sentiment, sort = T) 
```

```{r echo = F}
bing_counts %>%
  group_by(sentiment) %>%
  top_n(10, n) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x= word, y = n, fill = sentiment))+
  geom_col(show.legend = F) +
  facet_wrap(~sentiment, scales = "free_y")+
  labs(y = "Contribution to sentiment", x = NULL) + 
  coord_flip()
```


## Customize stop words

- For this analysis, we should consider removing the word 'trump' as a word with positive connotation
- To do so, we can make a custom list of stop-words:

```{r echo = T}
my_stop_words <- tibble(word = c("trump"), lexicon = c("custom"))
custom_stop_words <- bind_rows(my_stop_words, stop_words)
custom_stop_words
```

##  Most common positive/negative words with custom stop words 

```{r echo = F}
tidy_tweets %>%
  filter(!word %in% custom_stop_words $ word,
         !word %in% str_remove_all(custom_stop_words$word, "'"),
         str_detect(word, "[a-z]"))%>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(word, sentiment, sort = T) %>%
  group_by(sentiment) %>%
  top_n(10, n) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x= word, y = n, fill = sentiment))+
  geom_col(show.legend = F) +
  facet_wrap(~sentiment, scales = "free_y")+
  labs(y = "Contribution to sentiment", x = NULL) + 
  coord_flip()
```

## Wordclouds
-We can visualize the frequencies using a word cloud:

```{r wordcloud-code, cache = T, eval = F}
tidy_tweets %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 60,scale=c(3,.5)))
```
- Or using a sentiment word cloud:
```{r echo = F}
custom_tidy_tweets <- tidy_tweets %>%
  filter(!word %in%custom_stop_words$word,
         !word %in% str_remove_all(custom_stop_words$word, "'"),
         str_detect(word, "[a-z]")) 
```

```{r sentiment-wordcloud-code, warning = F, cache = T, eval = F}
custom_tidy_tweets%>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(word, sentiment, sort = T) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%   # turn data frame into matrix
  comparison.cloud(max.words = 60, scale=c(3,.5))
```

## Wordcloud
```{r wordcloud, cache = T, echo = F}
tidy_tweets %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 60,scale=c(3,.5)))
```

## Sentiment wordcloud
```{r sentiment-wordcloud, warning = F, cache = T, echo = F}
custom_tidy_tweets%>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(word, sentiment, sort = T) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%   # turn data frame into matrix
  comparison.cloud(max.words = 50, scale=c(3,.5))
```

# Comparing candidates

## Word frequencies of Biden vs. Warren
- We can visualize the differences and similarities in word frequencies between two candidates: 
```{r frequency-plot, warning=FALSE, eval = F}
# make into nice plot
frequency <- frequency %>%
  select(screen_name, word, freq) %>%
  spread(screen_name, freq) %>%
  arrange(BernieSanders, ewarren, JoeBiden, KamalaHarris)

# choose 2 candidates
ggplot(frequency2, aes(JoeBiden, ewarren))+
  # hide discreteness
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +
  geom_text(aes(label = word), check_overlap = T, vjust = 1.5)+
  scale_x_log10(labels = percent_format())+
  scale_y_log10(labels = percent_format())+
  geom_abline(color = "blue")

```

---

```{r echo = F, warning = F}
frequency <- frequency %>%
  select(screen_name, word, freq) %>%
  spread(screen_name, freq) %>%
  arrange(BernieSanders, ewarren, JoeBiden, KamalaHarris)

ggplot(frequency, aes(JoeBiden, ewarren))+
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +
  geom_text(aes(label = word), check_overlap = T, vjust = 1.5)+
  scale_x_log10(labels = percent_format())+
  scale_y_log10(labels = percent_format())+
  geom_abline(color = "blue")
```

- Words near the blue line are used with roughly equal frequencies between Joe Biden and Elizabeth Warren
- Words far away from the blue line are favored much more by one candidate than the other


## Term and inverse document frequency

- How to quantify what a document is about?
  - Term frequency (tf): how often a word occurs in the document
    - Not always the best measurement due to common words like "the", "and", etc. List of stop words is not sustainable
    - Term frequency will tend to incorrectly emphasize documents using the word "the"; "the" is not a good token to distinguish between tweets
  - Inverse document frequency (idf): increases weight of rare words and decreases weight of common words in a document 
    - Measure of how much information a word provides
    
## tf-idf

$$idf(\text{word}) = \log \left(\dfrac{\text{number of documents}}{\text{number of documents containing word}} \right)$$

- If every document contains the word, the fraction = 1, and so $idf = \log(1) = 0$. 
  - i.e. The word does not tell us much information about the document
- tf-idf is the product of tf and idf, and is intended to measure how important a word is relative to the document it came from

## high tf-idf 
- The `bind_tf_idf()` function takes a tidy text dataset as input and returns a dataframe with tf, idf, and tf-idf calculated
```{r echo = F}
tidy_tweets2 <- tidy_tweets %>%
  filter(!str_detect(word, "@")) %>%
  filter(!str_detect(word, "#")) %>%
  select(screen_name, word) 
```

- Which terms have high tf-idf?
```{r tf-idf}
candidate_tfidf <- tidy_tweets2 %>%
  count(screen_name, word, sort= T) %>%
  bind_tf_idf(word, screen_name, n)
candidate_tfidf %>%
  arrange(desc(tf_idf))
```
- This should make sense, because some names and hashtags are characteristic to a particular document/candidate

## Visualizing tf-idf
- Which terms have the highest tf-idf for the four candidates?

```{r tf-idf-plot, echo = F}
candidate_tfidf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  group_by(screen_name) %>%
  top_n(10, tf_idf) %>%
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = screen_name))+
  geom_col(show.legend =F) +
  labs(x = NULL, y = "tf-idf") + 
  facet_wrap(~screen_name, ncol= 2, scales= "free")+
  coord_flip()
```

## Probability of using a word
- We have examined which words have high tf-idf, that is, which words are distinctive for a particular candidate
- Now we can also explore questions such as: given a word, which candidate is more likely to use that word in a tweet?
- We will utilize the log-odds ratio 

## Odds ratio and log-odds ratio
- Below, we define the odds ratio for Candidate A versus Candidate B
$$\text{OR}_{A:B}(word) = \dfrac{\text{odds}_A}{\text{odds}_B} = \dfrac{\text{Prob(A uses word)}}{\text{Prob(B uses word)}}$$
- The probability that Candidate A uses the word is the number of times A used the word, divided by the total number of words used by A: 
$$\text{Prob(A uses word)} = \dfrac{n_A}{total_A}$$

## Log-odds ratio

$$ \log \text{OR}_{A:B}(word)  = \log \left( \dfrac{n_A / total_A}{n_B / total_B}\right) $$

- Equal odds corresponds to OR = 1, which corresponds to log(OR) = 0
- If candidate A uses the word with higher probability, then log(OR) > 0
- We use the following approximation in case a candidate does not use the word at all
$$ \log \text{OR}_{A:B}(word) \approx \log \left( \dfrac{(n+1)_A / (total+1)_A}{(n+1)_B / (total+1)_B}\right)$$


## Word usage: equally likely
-Here, we calculate log-odds ratios for Biden versus Warren and display them in ascending order of absolute value of log-odds ratio

```{r word-usage-likely, echo = F }
candidates <- c("JoeBiden", "ewarren")
word_ratios <- tidy_tweets %>%
  filter(screen_name %in% candidates) %>%
  filter(!str_detect(word, "^@")) %>%
  count(word, screen_name) %>%
  group_by(word) %>%
  filter(sum(n) >= 10) %>% # only consider more frequently uses words
  ungroup() %>%
  spread(screen_name, n, fill = 0) %>%
  mutate_if(is.numeric, list(~(. +1) / (sum(.) + 1))) %>%
  mutate(logratio = log(eval(parse(text = candidates[1])) / eval(parse(text = candidates[2])))) %>%
  select(word, logratio)%>%
  arrange(desc(logratio))
word_ratios %>%
  arrange(abs(logratio))
```

- Words about equally likely to be tweeted from the two candidates are non-buzzwords

## Word usage: most distinctive 
- The words that are most likely to be from Biden and not Warren will have the largest, most positive ratios
- The words that are most likely to be from Warren and not Biden will have the smallest, most negative ratios
- The plot on the next slides shows the 20 most positive and negative ratios

## Word usage: most distinctive cont.

```{r word-usage-diff, echo = F}
word_ratios %>% 
  group_by(logratio < 0) %>%
  top_n(20, abs(logratio)) %>%
  ungroup() %>%
  mutate(word = reorder(word, logratio))%>%
  ggplot(aes(word, logratio, fill = logratio < 0)) +
  geom_col(show.legend = F) +
  coord_flip() +
  ylab(paste("log odds ratio", candidates[1], "/", candidates[2])) +
  scale_fill_discrete(name = "", labels = c(candidates[1], candidates[2]))
```
- The most distinctive words are much more personal/characteristic of the candidate
  - Biden talks a lot about himself, whereas Warren talks more about her platform

# Beyond one-word tokens

## n-grams
- We can tokenize text into consecutive sequences of $n$ words called n-grams
  - One-word token is a `unigram', two consecutive word tokens are 'bigrams', etc.
  
```{r bigrams, eval = T, cache = T, warning = F}
tweet_bigrams <- tweets %>%
  select(screen_name, text) %>%
  mutate(text = str_remove_all(text, remove_reg))%>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
tweet_bigrams
```


## Counts of bigrams

```{r bigram-counts}
tweet_bigrams %>%
  count(bigram, sort = T)
```
 
## Remove custom stop_words
```{r http-stop-words}
my_stop_words <- tibble(word = c("http", "https"), lexicon = c("custom"))
custom_stop_words <- bind_rows(my_stop_words, stop_words)

bigrams_sep <- tweet_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ") 
bigrams_filtered <- bigrams_sep %>%
  filter(!word1 %in% custom_stop_words$word,
         !word1 %in% str_remove_all(custom_stop_words$word, "'"),
         str_detect(word1, "[a-z]"), 
         !word2 %in% custom_stop_words$word,
         !word2 %in% str_remove_all(custom_stop_words$word, "'"),
         str_detect(word2, "[a-z]"))
```

```{r custom-bigram-counts, echo =F}
bigram_counts <- bigrams_filtered %>%
  count(word1, word2, sort = T)
bigram_counts
```

## Words within a single tweet
- We may be interested in which words tend to appear in a single tweet (not necessarily consecutive pairs)
- For each candidate, we create a new variable called `rank` to indicate the order in which the tweets were published
- Then we count the number of word pairs that occur within a single tweet
```{r create-sections, eval = T, echo= F, warning = F}
tweet_section_words <-  tweets %>%
  mutate(text = str_remove_all(text, remove_reg)) %>%
  select(screen_name, text, timestamp) %>%
  group_by(screen_name) %>%
  # create notion of section/single tweet
  mutate(rank = rank(desc(timestamp)) )%>%
  ungroup() %>%
  unnest_tokens(word, text, token = "tweets") %>%
  filter(!word %in% stop_words $ word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]"))
```

## Word pairs 

```{r word-pairs}
word_pairs <- tweet_section_words %>%
  pairwise_count(word, rank, sort= T )
word_pairs
```

## Pairwise correlations
- We see that "health" and "care" are the two most common co-occurring words within a single tweet, followed by "country" and "people"
- However, you may recall that those words are also among the most common individual unigrams
- Therefore, it may be more meaningful to measure how often pairs of words are used together relative to how often they are used individually
- This is the correlation among words

## Phi coefficient  
- We will use the phi coefficient, which focuses on how much more likely it is that either both/neither of two words appear, versus only one of the two appearing
- A correlation close to 0 means that the two words are not likely to co-occur, and a correlation close to $\pm 1$ means that the two are both likely to simultanegously occur or not occur
- We will focus on the most common individual words

```{r}
word_cors <- tweet_section_words %>%
  group_by(word) %>%
  filter(n() >= 75) %>%
  pairwise_cor(word, rank, sort = T)
```


## Most positively correlated pairs
- The most positively correlated pairs of words are common two-word terms: health care, student debt, etc.

```{r pairwise-cor-high}
word_cors %>%
  slice(1:10)
```

## Most negatively correlated pairs
- No pairs of words are extremely negatively correlated
```{r}
word_cors %>% tail()
```

## Pairwise correlation cont.
- When we looked at pairwise counts, "people" and "country" occurred frequently in a single tweet. But how correlated are they?
```{r pairwise-cor-low}
word_cors  %>%
  filter(item1 == "people" & item2 == "country")
```

- So even though "people" and "country" co-occur very often, they are not very associated. 
- That is, these two words often occur together, but many tweets use one word and not the other

## Graph of highly correlated words
- Lastly, we can visualize the correlations and clusters of high, positively correlated word pairs
```{r corr-graph-code, eval = F}
word_cors %>%
  filter(correlation > .15) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```

---

```{r corr-graph, echo = F}
word_cors %>%
  filter(correlation > .15) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```

# Questions?
