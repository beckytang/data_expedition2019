---
title: "Text Analysis with R"
author: "Becky Tang and Graham Tierney"
date: "October 3, 2019"
output:
  beamer_presentation:
    theme: "Madrid"
    colortheme: "seahorse"
    fonttheme: "structurebold"
editor_options: 
  chunk_output_type: console
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
library(tidyverse)
library(tidytext)
library(stringr)
library(tidyr)
library(scales)
library(textdata)
library(wordcloud)
library(reshape2)
library(janeaustenr)
library(lubridate)
library(scales)

```

# Introduction 
 

## What is text analysis?
- As statisticians and data scientistis, why restrict ourselves to numeric data?
- We can use text as data and employ statistical methods to derive patterns or trends 
  - Example: Given some song lyrics, can we determine who the singer is? What kinds of topics/sentiments does Lizzo like to sing about?
  
## Tidytext
- So far in class, you have been employing tidy data principles, and we will continue to do so here!
- Specifically, we will work in the 'tidy text format': a table with one \textbf{token} per row
```{r introduce_text}
pp <- austen_books() %>%
  filter(book == "Pride & Prejudice") %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>%
  select(-"book")
pp %>% slice(1:11)
```
- How can we make this text into a format we can work with?

## Tokenizing
- A \textbf{token} is a meanginful unit of text. For us, that will be a single word
- Tokenizing is simply splitting a body of text into tokens, which can achieved using the function `unnest_tokens()`

```{r unnest-token, echo = T}
tidy_pp <- pp %>%
  unnest_tokens(word, text)
tidy_pp
```

## Tokenizing cont.
- With the `unnest_tokens()` function, we can easily format any body of text into a user-friendly data frame
- First argument: name of column/variable that text is being unnested into
- Second argument: name of column/variable that is to be unnested 
- Other details:
  - All other columns kept
  - Punctuation removed
  - Defaults is to convert tokens to lowercase

# Twitter Data

## Democratic candidate tweets
- Time to work with some fun data!
- We have pulled tweets from four Democratic candidates: Joe Biden, Kamala Harris, Bernie Sanders, and Elizabeth Warren
```{r}
setwd("~/Documents/data_expedition2019/data")
tweets <- read.csv("dem_cand_tweets.csv")
#colnames(tweets)
tweets <- tweets %>%
  mutate(timestamp = ymd_hms(created_at)) %>%
  select("created_at", "screen_name", "text", "is_retweet", "retweet_count", "timestamp")
tweets %>%
  select("screen_name", "text") %>%
  slice(1:2)
```

## Tidy tweets
```{r}
remove_reg <- "&amp;|&lt;|&gt;"
tidy_tweets <- tweets %>%
  #get rid of retweets
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = str_remove_all(text, remove_reg))%>%
  unnest_tokens(word, text, token = "tweets")
head(tidy_tweets)
```

## Word counts
- What are the most commonly tweeted words by these candidates?
```{r}
tidy_tweets %>%
  count(word, sort = T)
```

## Stop words
```{r, echo = T}
data("stop_words")
head(stop_words)
tidy_tweets <- tidy_tweets %>%
  filter(!word %in% stop_words $ word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]"))
tidy_tweets %>%
  count(word, sort = T)
tidy_tweets %>%
  group_by(screen_name) %>%
  count(word, sort = T)
```

## Word frequency
- While it's nice to get raw counts, but we may prefer to know which words are used more often \textit{relative} to other words. For this we can look towards word frequencies:

```{r frequency}
frequency_all <-  tidy_tweets %>%
  count(word, sort = T) %>%
  mutate(freq = n / n()) 

ggplot(frequency_all %>% top_n(15) %>%
         mutate(word = reorder(word, freq)), aes(x = word, y = freq))+
  geom_col()+
  coord_flip() 

frequency <- tidy_tweets %>%
  group_by(screen_name) %>%
  count(word, sort = T) %>%
  left_join(tidy_tweets %>%
              group_by(screen_name) %>%
              summarise(total = n()),
            by = "screen_name") %>%
  mutate(freq = n / total)

frequency %>%
  head()
```

## Plot of word frequency
- We can visualize the differences and similarities in word frequencies between two candidates: 
```{r frequency-plot, warning=FALSE}
# make into nice plot
frequency <- frequency %>%
  select(screen_name, word, freq) %>%
  spread(screen_name, freq) %>%
  arrange(BernieSanders, ewarren, JoeBiden, KamalaHarris)

# choose 2 candidates
ggplot(frequency, aes(JoeBiden, ewarren))+
  # hide discreteness
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +
  geom_text(aes(label = word), check_overlap = T, vjust = 1.5)+
  scale_x_log10(labels = percent_format())+
  scale_y_log10(labels = percent_format())+
  geom_abline(color = "blue")
```
- Words near the blue line are used with roughly equal frequencies between Joe Biden and Elizabeth Warren
- Words far away from the blue line are favored much more by one candidate than the other


## Sentiment analysis

- Now that we have looked at a per-word/token level, we may be interested in determining the general emotion of the text/author
- We can consider a word to have a particular emotion attached to it
- We can consider a piece of text as a combination of its individual words, so the sentiment of an entire text is the sum of the sentiments of its individual words
- There are three general-purpose sentiment lexicons: AFINN, bing, and nrc

## AFINN

- AFINN assigns each word an integer score between -5 and 5
- Negative scores indicate negative sentiment, and positive scores indicate the opposite

```{r}
get_sentiments("afinn") 
```

## bing

- The bing lexicon categorizes words into one of two categories: 'positive' or 'negative'
```{r}
get_sentiments("bing") 
```

## Important notes about sentiment lexicons
- Not every English word is in a lexicon
```{r}
get_sentiments("bing") %>%
  filter(word == "data")
```
- These lexicons do not account for qualifiers before a word (ex. "not happy") because these lexicons are constructed for one-word tokens only
- If we sum up each word's sentiment, it is possible for a piece of text to average out to a sentiment of zero/neutral

## Sentiment of Elizabeth Warren's tweets: bing lexicon

- We will use the bing lexicon to estimate the sentiment of Elizabeth Warren's tweets:

```{r}
tidy_tweets %>%
  filter(screen_name == "ewarren") %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(sentiment) %>%
  spread(sentiment, n) %>%
  mutate(sentiment = positive - negative)
```

- It appears that Warren is pretty neutral...or is she?

## Sentiment of Elizabeth Warren's tweets by month

- What if we look at Warren's tweets by month?

```{r}
tidy_tweets %>%
  filter(screen_name == "ewarren") %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(months(timestamp), sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

```

## Most common positive/negative words

```{r}
bing_counts <- tidy_tweets %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(word, sentiment, sort = T) 
```

```{r echo = F}
bing_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x= word, y = n, fill = sentiment))+
  geom_col(show.legend = F) +
  facet_wrap(~sentiment, scales = "free_y")+
  labs(y = "Contribution to sentiment", x = NULL) + 
  coord_flip()
```

## Customize stop words

- For this analysis, we should consider removing the word 'trump' as a word with positive connotation
- To do so, we can make a custom list of stop-words:

```{r}
my_stop_words <- tibble(word = c("trump"), lexicon = c("custom"))
custom_stop_words <- bind_rows(my_stop_words, stop_words)
custom_stop_words
```

```{r, echo = F}
tidy_tweets %>%
  filter(!word %in% custom_stop_words $ word,
         !word %in% str_remove_all(custom_stop_words$word, "'"),
         str_detect(word, "[a-z]"))%>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(word, sentiment, sort = T) %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x= word, y = n, fill = sentiment))+
  geom_col(show.legend = F) +
  facet_wrap(~sentiment, scales = "free_y")+
  labs(y = "Contribution to sentiment", x = NULL) + 
  coord_flip()
```

## Log-odds ratio
- Looking beyond raw frequencies, we can now begin to investigate which candidate is more likely to use a given word in a tweet
- To do so, we will utilize the \textbf{log-odds ratio}
$$\text{OR}_{A:B} = \dfrac{\text{odds}_A}{\text{odds}_B} = \dfrac{\text{prob}(word)_A}{\text{prob}(word)_B}$$
\begin{align*}
\log \text{OR}_{A:B} = \log \left( \dfrac{\text{prob}(word)_A}{\text{prob}(word)_B}\right) &= \log \left( \dfrac{n_A / total_A}{n_B / total_B}\right) \\ 
& \approx \log \left( \dfrac{(n+1)_A / (total+1)_A}{(n+1)_B / (total+1)_B}\right)
\end{align*}

## Word usage

```{r word usage}
candidates <- c("JoeBiden", "KamalaHarris")
word_ratios <- tidy_tweets %>%
  filter(screen_name %in% candidates) %>%
  filter(!str_detect(word, "^@")) %>%
  count(word, screen_name) %>%
  group_by(word) %>%
  filter(sum(n) >= 10) %>%
  ungroup() %>%
  spread(screen_name, n, fill = 0) %>%
  mutate_if(is.numeric, list(~(. +1) / sum(.) + 1)) %>%
  mutate(logratio = log(eval(parse(text = candidates[1])) / eval(parse(text = candidates[2])))) %>%
  arrange(desc(logratio))

```
